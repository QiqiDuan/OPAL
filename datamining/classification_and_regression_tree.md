# 决策树与回归树

核心思想：递归二元划分。

代表性算法：```CART```、```ID3```、```C4.5```、```C5.0```、```Random Forests```等。

决策树主要用于预测目录型因变量，回归树主要用于预测连续性因变量。

## 决策树

信息增益计算方法：

* 基尼系数：可导、便于数值优化，主要用于树增长阶段

* 熵：可导、便于数值优化，主要用于树增长阶段

* 误分率：不可导，主要用于树剪枝阶段

## 回归树

目标函数构造方法：

* 均方误差

* 加权均方误差

发现最好的二分划分最小化均方误差在计算上是不可行的，因此寻求贪婪算法进行优化。

## 防止过度拟合

* 设置树节点所包含的最少样本数

* 设置树节点纯度或误差阈值：容易造成```短视```行为；当前低效的二元划分后续仍可能导致高效的划分

* 设置树最大深度

* 基于交叉验证的剪枝操作：```CART```所采取的策略

最优的树结构应该根据具体数据进行动态调整，利用交叉验证设置模型核心参数。

## 应用注意事项

* 内在偏向选择多目录的目录型变量进行划分

* 处理具有缺失值的预测变量：1).将缺失值视为一个目录数值；2).构造代理变量

* 在实践中，二元划分一般要优于多元划分

* 不稳定性：由其内在的```高方差```所导致

* 缺乏平滑能力，难以识别可加结构

## 参考文献

[Classification and regression trees](http://www.nature.com/nmeth/journal/v14/n8/full/nmeth.4370.html)

[9.2 Tree-Based Methods, The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf)
